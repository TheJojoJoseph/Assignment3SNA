{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDz3GNWDZKswp7czUwG9Fv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheJojoJoseph/Assignment3SNA/blob/main/Assignment3SNA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **1. Loading Datasets: **The gml_to_dataframe function is used to load the datasets (dolphins.gml, karate.gml, and football.gml). This method reads GML files and converts them to Pandas DataFrames that represent the graph's edges.\n",
        "2. **2. Data Cleaning: **To standardise the dataset, the function clean_dataset_fixed extracts only the first two columns (node1, node2) that indicate the edges and verifies that the data is of the integer type.\n",
        "convert_cleaned_to_adjacency_matrix: This function creates adjacency matrices from cleaned datasets. When an edge connects two nodes, this matrix will have 1; otherwise, it will have 0.\n",
        "3. **infoNode algorithm,** calculate_internal_force, determines the internal force between two nodes by utilising their shared neighbours and Jaccard coefficient.\n",
        "infonode with internal force: This program uses the InfoNode algorithm to find communities.\n",
        "locates high degree centrality seed nodes.\n",
        "By choosing the top-k neighbours with the highest internal force, communities are expanded.\n",
        "Communities that have a lot in common are combined to prevent over-segmentation.\n",
        "4. **Girvan-Newman community detection technique** is implemented by girvan_newman_algo, which repeatedly eliminates edges with the highest betweenness centrality until communities are formed.\n",
        "5. **Flattening Communities:**\n",
        "flatten_communities_safe: Flattens community structures into a list where each node is labeled with its community ID, ensuring no out-of-range errors.\n",
        "6. Network Statistics:\n",
        "compute_network_stats: Computes basic statistics for each graph (number of nodes, edges, average degree).\n",
        "network_stats_from_adj: Helper function that applies the above statistics calculation to an adjacency matrix.\n",
        "7. **Community Detection on Three Datasets:**\n",
        "Runs both the InfoNode and Girvan-Newman algorithms on the dolphins, karate, and football datasets, producing communities for each.\n",
        "8. **NMI Calculation:**\n",
        "NMI (Normalized Mutual Information) is used to compare the similarity between the community structures produced by the two algorithms.\n",
        "For each dataset (Dolphins, Karate, Football):\n",
        "Communities from InfoNode and Girvan-Newman are flattened into label vectors.\n",
        "The NMI between the two community label vectors is computed to quantify the similarity between the detected communities."
      ],
      "metadata": {
        "id": "AY9Fl4UiAjJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3 datasets (Dolphins, Karate, Football) -  Added in GITfile and Zip\n",
        "\n",
        "# Loading\n",
        "from google.colab import files\n",
        "# uploaded = files.upload() # I have already uploaded so commented.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community import girvan_newman\n",
        "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
        "\n",
        "\n",
        "def gml_to_dataframe(file_path):  #<--Load the GML file and convert to a DataFrame\n",
        "    G = nx.read_gml(file_path, label=None)  # Use label=None to avoid label errors\n",
        "    edges = [(u, v) for u, v in G.edges()]\n",
        "    return pd.DataFrame(edges, columns=['node1', 'node2'])\n",
        "\n",
        "dolphins_df = gml_to_dataframe('dolphins.gml')\n",
        "karate_df = gml_to_dataframe('karate.gml')\n",
        "football_df = gml_to_dataframe('football.gml')\n",
        "\n",
        "print('Dolphins_DataSet',dolphins_df.head())\n",
        "print('Karate_DataSet',karate_df.head())\n",
        "print('Football_DataSet',football_df.head())\n",
        "\n",
        "\n",
        "# Load datasets - If CSV\n",
        "# dolphins_df = pd.read_csv('dolphins.csv', header=None)\n",
        "# karate_df = pd.read_csv('karate.csv', header=None)\n",
        "# football_df = pd.read_csv('football.csv', header=None)\n",
        "\n",
        "def clean_dataset_fixed(data, has_weights=False): # Function to clean and convert the dataset into adjacency matrix\n",
        "    cleaned_data = data.copy().reset_index(drop=True)\n",
        "    if cleaned_data.dtypes.iloc[0] == 'object':\n",
        "        cleaned_data = cleaned_data[cleaned_data.columns[0]].str.strip().str.split(expand=True)\n",
        "    cleaned_data = cleaned_data.iloc[:, :2]\n",
        "    cleaned_data.columns = ['node1', 'node2']\n",
        "    return cleaned_data.astype(int)\n",
        "\n",
        "def convert_cleaned_to_adjacency_matrix(cleaned_data):  # -->Convert cleaned datasets to adjacency matrices\n",
        "    \"\"\"Converts cleaned dataset into an adjacency matrix.\"\"\"\n",
        "    max_node = max(cleaned_data['node1'].max(), cleaned_data['node2'].max())\n",
        "    adj_matrix = pd.DataFrame(0, index=range(max_node+1), columns=range(max_node+1))\n",
        "    for _, row in cleaned_data.iterrows():\n",
        "        adj_matrix.iloc[row['node1'], row['node2']] = 1\n",
        "        adj_matrix.iloc[row['node2'], row['node1']] = 1  # Symmetric\n",
        "    return adj_matrix\n",
        "\n",
        "# Clean the datasets && adjacency matrices\n",
        "dolphins_cleaned = clean_dataset_fixed(dolphins_df)\n",
        "karate_cleaned = clean_dataset_fixed(karate_df)\n",
        "football_cleaned = clean_dataset_fixed(football_df)\n",
        "\n",
        "dolphins_adj_matrix = convert_cleaned_to_adjacency_matrix(dolphins_cleaned)\n",
        "karate_adj_matrix = convert_cleaned_to_adjacency_matrix(karate_cleaned)\n",
        "football_adj_matrix = convert_cleaned_to_adjacency_matrix(football_cleaned)\n",
        "\n",
        "# InfoNode Algorithm\n",
        "def calculate_internal_force(G, node1, node2):\n",
        "    degree_product = G.degree(node1) * G.degree(node2)\n",
        "    neighbors1 = set(G.neighbors(node1))\n",
        "    neighbors2 = set(G.neighbors(node2))\n",
        "    intersection = len(neighbors1 & neighbors2)\n",
        "    union = len(neighbors1 | neighbors2)\n",
        "    jaccard_coefficient = intersection / union if union > 0 else 0\n",
        "    return degree_product * jaccard_coefficient\n",
        "\n",
        "def infonode_with_internal_force(adj_matrix, k=5):\n",
        "    G = nx.from_pandas_adjacency(adj_matrix)\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "    avg_centrality = np.mean(list(degree_centrality.values()))\n",
        "    seeds = [node for node, centrality in degree_centrality.items() if centrality > avg_centrality]\n",
        "    communities = {seed: [seed] for seed in seeds}\n",
        "    visited = set(seeds)\n",
        "    for seed in seeds:\n",
        "        neighbors = list(set(G.neighbors(seed)) - visited)\n",
        "        if not neighbors:\n",
        "            continue\n",
        "        internal_forces = [(neighbor, calculate_internal_force(G, seed, neighbor)) for neighbor in neighbors]\n",
        "        internal_forces = sorted(internal_forces, key=lambda x: -x[1])  # Sort by descending internal force\n",
        "        top_k_neighbors = [n for n, _ in internal_forces[:k]]\n",
        "        communities[seed].extend(top_k_neighbors)\n",
        "        visited.update(top_k_neighbors)\n",
        "    community_list = list(communities.values()) #Merge communities if they overlap significantly to reduce over-segmentation\n",
        "    merged_communities = []\n",
        "    while community_list:\n",
        "        current_community = set(community_list.pop(0))\n",
        "        merged = False\n",
        "        for i, other_community in enumerate(community_list):\n",
        "            if len(current_community & set(other_community)) > 0:\n",
        "                current_community.update(other_community)\n",
        "                community_list.pop(i)\n",
        "                merged = True\n",
        "                break\n",
        "        merged_communities.append(list(current_community))\n",
        "    return merged_communities\n",
        "\n",
        "# Girvan-Newman Algorithm\n",
        "def girvan_newman_algo(adj_matrix):\n",
        "    G = nx.from_pandas_adjacency(adj_matrix)\n",
        "    comp = girvan_newman(G)\n",
        "    communities = tuple(sorted(c) for c in next(comp))\n",
        "    return communities\n",
        "\n",
        "#  flatten communities helper for NMI comparison\n",
        "def flatten_communities(communities, num_nodes):\n",
        "    labels = [-1] * num_nodes\n",
        "    for comm_id, nodes in communities.items():\n",
        "        for node in nodes:\n",
        "            labels[node] = comm_id\n",
        "    return labels\n",
        "\n",
        "def flatten_communities_list(communities, num_nodes):\n",
        "    labels = [-1] * num_nodes\n",
        "    for comm_id, nodes in enumerate(communities):\n",
        "        for node in nodes:\n",
        "            labels[node] = comm_id\n",
        "    return labels\n",
        "\n",
        "def compute_network_stats(G): #basic network statistics\n",
        "    num_nodes = G.number_of_nodes()\n",
        "    num_edges = G.number_of_edges()\n",
        "    avg_degree = sum(dict(G.degree()).values()) / num_nodes\n",
        "    return num_nodes, num_edges, avg_degree\n",
        "\n",
        "#Iterator\n",
        "def network_stats_from_adj(adj_matrix):\n",
        "    G = nx.from_pandas_adjacency(adj_matrix)\n",
        "    return compute_network_stats(G)\n",
        "\n",
        "# Flatten communities for NMI comparison\n",
        "def flatten_communities_safe(communities, num_nodes):\n",
        "    \"\"\"Flattens community structure into a list where index is the node and value is the community label, ensuring correct indexing.\"\"\"\n",
        "    labels = [-1] * (num_nodes + 1)  # Ensure size accommodates for all nodes, even if non-continuous\n",
        "    if isinstance(communities, dict):\n",
        "        for comm_id, nodes in communities.items():\n",
        "            for node in nodes:\n",
        "                if node < len(labels):\n",
        "                    labels[node] = comm_id\n",
        "    else:\n",
        "        for comm_id, nodes in enumerate(communities):\n",
        "            for node in nodes:\n",
        "                if node < len(labels):\n",
        "                    labels[node] = comm_id\n",
        "    return labels[:num_nodes]\n",
        "\n",
        "# --------------- Apply the algorithms and calculate NMI for Dolphins, Karate, and Football ---------------------------------\n",
        "# InfoNode for Dolphins\n",
        "infonode_communities_dolphins = infonode_with_internal_force(dolphins_adj_matrix)\n",
        "gn_communities_dolphins = girvan_newman_algo(dolphins_adj_matrix)\n",
        "\n",
        "# InfoNode for Karate\n",
        "infonode_communities_karate = infonode_with_internal_force(karate_adj_matrix)\n",
        "gn_communities_karate = girvan_newman_algo(karate_adj_matrix)\n",
        "\n",
        "# InfoNode for Football\n",
        "infonode_communities_football = infonode_with_internal_force(football_adj_matrix)\n",
        "gn_communities_football = girvan_newman_algo(football_adj_matrix)\n",
        "\n",
        "# Network stats for Dolphins\n",
        "dolphins_stats = network_stats_from_adj(dolphins_adj_matrix)\n",
        "dolphins_info_communities = len(infonode_communities_dolphins)\n",
        "dolphins_gn_communities = len(gn_communities_dolphins)\n",
        "\n",
        "# Network stats for Karate\n",
        "karate_stats = network_stats_from_adj(karate_adj_matrix)\n",
        "karate_info_communities = len(infonode_communities_karate)\n",
        "karate_gn_communities = len(gn_communities_karate)\n",
        "\n",
        "# Network stats for Football\n",
        "football_stats = network_stats_from_adj(football_adj_matrix)\n",
        "football_info_communities = len(infonode_communities_football)\n",
        "football_gn_communities = len(gn_communities_football)\n",
        "\n",
        "# Flatten communities for NMI comparison\n",
        "num_nodes_dolphins_result = dolphins_adj_matrix.shape[0]\n",
        "infonode_labels_dolphins = flatten_communities_safe(infonode_communities_dolphins, num_nodes_dolphins_result)\n",
        "gn_labels_dolphins = flatten_communities_safe(gn_communities_dolphins, num_nodes_dolphins_result)\n",
        "nmi_dolphins = nmi(infonode_labels_dolphins, gn_labels_dolphins)\n",
        "\n",
        "num_nodes_karate_result = karate_adj_matrix.shape[0]\n",
        "infonode_labels_karate = flatten_communities_safe(infonode_communities_karate, num_nodes_karate_result)\n",
        "gn_labels_karate = flatten_communities_safe(gn_communities_karate, num_nodes_karate_result)\n",
        "nmi_karate = nmi(infonode_labels_karate, gn_labels_karate)\n",
        "\n",
        "num_nodes_football_result = football_adj_matrix.shape[0]\n",
        "infonode_labels_football = flatten_communities_safe(infonode_communities_football, num_nodes_football_result)\n",
        "gn_labels_football = flatten_communities_safe(gn_communities_football, num_nodes_football_result)\n",
        "nmi_football = nmi(infonode_labels_football, gn_labels_football)\n",
        "\n",
        "def print_nmi(dataset_name, results):\n",
        "    print(f\"{dataset_name} Dataset: NMI = {results}\")\n",
        "\n",
        "# --------------------   NMI results for all three datasets. ----------------------\n",
        "print_nmi(\"Dolphins\", round(nmi_dolphins, 4))\n",
        "print_nmi(\"Karate\", round(nmi_karate, 4))\n",
        "print_nmi(\"Football\", round(nmi_football, 4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyWIFcgk3PhI",
        "outputId": "55db0bac-ba52-4e45-a9ca-98719e9a3a46"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dolphins_DataSet    node1  node2\n",
            "0      0     10\n",
            "1      0     14\n",
            "2      0     15\n",
            "3      0     40\n",
            "4      0     42\n",
            "Karate_DataSet    node1  node2\n",
            "0      1      2\n",
            "1      1      3\n",
            "2      1      4\n",
            "3      1      5\n",
            "4      1      6\n",
            "Football_DataSet    node1  node2\n",
            "0      0      1\n",
            "1      0      4\n",
            "2      0      9\n",
            "3      0     16\n",
            "4      0     23\n",
            "Dolphins Dataset: NMI = 0.2681\n",
            "Karate Dataset: NMI = 0.4407\n",
            "Football Dataset: NMI = 0.2253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qXjLyH-sAiIn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mAi7Atnh2piX"
      },
      "outputs": [],
      "source": []
    }
  ]
}